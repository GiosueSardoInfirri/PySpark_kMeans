{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k means ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://10.67.22.124:7077\")\\\n",
    "    .appName(\"prova iniziale\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# print its status\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from sklearn (load in the disk)\n",
    "\n",
    "This is going to be removed in the Cloud Veneto version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\") # To hide warnings, commented to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn # to be run at the launch of \"docker compose up\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.datasets #va installato\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import least\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from operator import add\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "k = 30\n",
    "G = 5 # multiplying factor\n",
    "len_df = 50_000 # 494_021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# # Import dataframe directly from sklearn\n",
    "# prova =  sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True)\n",
    "# X = prova.data\n",
    "\n",
    "# n_cols = len(X.iloc[0])\n",
    "\n",
    "# n_cols = n_cols - 3 #because we will remove the binary variables\n",
    "\n",
    "# # Create the spark dataframe with a smaller chunk of data (just to work easily)\n",
    "# X_smaller = X.iloc[random.sample(range(0, len(X.index)), len_df)]\n",
    "# spark_X = spark.createDataFrame(X_smaller)\n",
    "\n",
    "# spark_X = spark_X.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spark dataframe\n",
    "And then acquire important informations about the dataframe: # columns, # rows, # partitions, the schema, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spark_X = spark.createDataFrame(sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True)['frame'].iloc[random.sample(range(0, len_df), len_df)])\n",
    "# spark_X = spark.createDataFrame(sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True)['frame'])\n",
    "spark_X = spark_X.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = spark_X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of partitions the DataFrame is divided into the three workers yet\n",
    "spark_X.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that is able to select the single row in the dataframe\n",
    "def getrows(df, rownums=None):\n",
    "    return df.rdd.zipWithIndex().filter(lambda x: x[1] in rownums).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the non-numerical values in an easy way\n",
    "# clean_X = spark_X.select('*').drop('protocol_type', 'service', 'flag')\n",
    "# clean_X.printSchema()\n",
    "\n",
    "# Delete the non-numerical values in a reusable way\n",
    "col_type = np.array(spark_X.dtypes)\n",
    "types = col_type[:,1] \n",
    "colnames = col_type[:,0]\n",
    "clean_X = spark_X.select([col(colnames[i]) for i in range(len(colnames)) if not types[i] == 'binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of rows and columns in the dataframe\n",
    "n_rows = len_df\n",
    "n_cols = len(getrows(clean_X, rownums=[0]).collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_X.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Useful functions for the initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function for the distance between the cluster centers and the pandas dataframe\n",
    "def distance(xrow, centers, num_cols=n_cols):\n",
    "    '''Distance between a dataframe.row\n",
    "    and the broadcasted value list of centers\n",
    "    in the form of dataframe.row.value:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows).value\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    x = np.array(xrow)[:num_cols]\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])[:num_cols]\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.min(the_ds)\n",
    "\n",
    "# Formula to evaluate the oversamping factor with the pre-factor G\n",
    "def evaluate_l(log_phi, k, G):\n",
    "    return G * k/log_phi # G = over-oversampling factor\n",
    "\n",
    "# Function to select a row based on its probability\n",
    "def select_row(x):\n",
    "    if x > np.random.uniform(low = 0, high = 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is NOT $\\Phi$. This is $log(\\Phi)$. Pay attention budeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose a random sample from the dataset\n",
    "This is the required step to begin the algorithm (doesn't need to be parallelized, since it is a select task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the first sample randomly: select the random row\n",
    "random_n = [np.random.randint(0, n_rows)]\n",
    "random_sample = getrows(clean_X, random_n).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initial cost\n",
    "Then we evaluate the cost function (sum of the squares after the first selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "colnames = list(clean_X.dtypes[i][0] for i in range(len(clean_X.dtypes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "clean_X = clean_X.withColumn(\"minimum_cost\", lit(0))\n",
    "for colname in colnames:\n",
    "    clean_X = clean_X.withColumn(\"minimum_cost\", clean_X['minimum_cost'] + (clean_X[colname]-random_sample[0][colname])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add the colunm to keep the minimum distance between each point and the closest center\n",
    "clean_X = (clean_X.select('*')\n",
    "           .withColumn('minimum_cost', sum((col(colname)-random_sample[0][colname])**2 for colname in colnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = sum((col(colname)-random_sample[0][colname])**2 for colname in colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initial cost\n",
    "initial_cost = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "initial_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add the newly introduced value to the row that I selected in the first place\n",
    "temp = random_sample[0].asDict()\n",
    "temp[\"minimum_cost\"] = 0.\n",
    "random_sample[0] = Row(**temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Broadcasting the row over the workers\n",
    "bCent = sc.broadcast(random_sample)\n",
    "\n",
    "# To show the content of the broadcast: b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: evaluate number of iterations\n",
    "\n",
    "After having evaluated $\\log \\phi$, we find a number of iterations which is of the same order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_iter = int(initial_cost)\n",
    "n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: For loop\n",
    "Implement the for loop in order to evaluate probabilities and choose new centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "phi_iter = initial_cost\n",
    "l = evaluate_l(phi_iter, k, G)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "last_centers = 1\n",
    "start_time = time.time()\n",
    "\n",
    "while i < n_iter:   # or len(bCent.value) < k: # added as a safety condition (useless for very long datasets)\n",
    "\n",
    "        '''\n",
    "        Nel ciclo for:\n",
    "            - Evaluate for each row l * d()^2 / phi\n",
    "            - Sample with that probability\n",
    "            - Broadcast centers to nodes\n",
    "            - Evaluate new cost\n",
    "\n",
    "        Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "        '''\n",
    "\n",
    "        # Evaluate the probability and select the new rows\n",
    "        mod_phi = l/np.exp(phi_iter)\n",
    "        new_rows = clean_X.select('*').withColumn('random_number', rand(seed=int(time.time())))\\\n",
    "                          .filter(col('random_number') < col('minimum_cost')*mod_phi).drop('random_number').collect()\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Update the broadcast in a non-usual way (newest at the beginning)\n",
    "        bCent = sc.broadcast(new_rows + bCent.value)\n",
    "        \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Update the minimum distance\n",
    "        if len(new_rows) == 1:\n",
    "            clean_X = clean_X.select('*').\\\n",
    "                      withColumn('minimum_cost', least('minimum_cost', reduce(add, [(col(colname)-bCent.value[0][colname])**2 for colname in colnames]))).cache()\n",
    "\n",
    "        elif len(new_rows) > 1:\n",
    "            clean_X = clean_X.select('*').withColumn('dummy', least(*[reduce(add, [(col(colname)-bCent.value[center][colname])**2 for colname in colnames]) for center in range(len(new_rows))] )).\\\n",
    "                      withColumn('minimum_cost', least('minimum_cost', 'dummy')).\\\n",
    "                      drop('dummy').cache()\n",
    "\n",
    "        last_centers = len(bCent.value)\n",
    "\n",
    "        # Evaluate new cost\n",
    "        phi_iter = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        print(\"\\n at iteration\", i ,\", #values: \",len(bCent.value), \" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each row evaluate distance from new points --> if it's less than mindistance substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "clean_X.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Select a subset of the possible centroids using k-means ++\n",
    "\n",
    "Using kmeans ++:\n",
    "\n",
    "Algorithm 1 k-means++(k) initialization.\n",
    "1: C ← sample a point uniformly at random from X\n",
    "2: while |C| < k do\n",
    "2\n",
    "3:\n",
    "Sample x ∈ X with probability dφX(x,C)\n",
    "(C)\n",
    "4:\n",
    "C ← C ∪ {"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to find closest center among list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function to find the closest center from any point in order to find the weights\n",
    "def find_closest_center(xrow, broadC, num_cols=n_cols):\n",
    "    '''Find the center in the broadcasted value \n",
    "    list whose distance from a dataframe.row \n",
    "    is the lowest:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows)\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    x = np.array(xrow)[:num_cols]\n",
    "    centers = broadC.value\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])[:num_cols]\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.argmin(the_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dfCent = spark.createDataFrame(bCent.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "over_sampled_centers = dfCent.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dfCent.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "len(bCent.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce size of dfCent to have only `k` centroids \n",
    "\n",
    "#### Step 1: calc weights $w_x$\n",
    "set wx to be the number of points in X closer to x than any other point in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wx = clean_X.rdd.map(lambda row: (find_closest_center(row,bCent), 1)).\\\n",
    "        reduceByKey(lambda x,y: x+y).\\\n",
    "        takeOrdered(over_sampled_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "type(wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wx = (np.array(wx)[:,1]).astype(float)\n",
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wx /= np.sum(wx)\n",
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Maybe we need to get rid of bCent ??\n",
    "# bCent.destroy()\n",
    "# bCent.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use weighted `k-means++`\n",
    "    2.1 Draw 1 random center\n",
    "    2.2 update cost function\n",
    "    2.3 repeat 1-2 until happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Draw 1 random center\n",
    "\n",
    "first_index = np.random.choice(a=range(wx.shape[0]), size=1, p=wx) \n",
    "#first point sampled uniformly wrt distance\n",
    "\n",
    "first_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "first_center = getrows(dfCent, first_index).collect()\n",
    "first_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Adds the first center to the ultimate center\n",
    "bCent_ultimate = sc.broadcast(first_center)\n",
    "bCent_ultimate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define the total distance from other centers (minimum cost) over the chosen center\n",
    "dfCent = (dfCent.select('*')\n",
    "         .withColumn('minimum_cost', sum((col(colname)-first_center[0][colname])**2 for colname in colnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the total cost value\n",
    "phi0 = np.log(dfCent.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "phi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2.2 Update cost function\n",
    "\n",
    "ultimate_sample_n = len(bCent_ultimate.value)\n",
    "\n",
    "while ultimate_sample_n < k: #n_iter or len(bCent.value) < k:\n",
    "    '''\n",
    "    Nel ciclo for:\n",
    "        - Evaluate for each row wx * d()^2 / phi\n",
    "        - Sample with that probability\n",
    "        - Broadcast centers to nodes\n",
    "        - Evaluate new cost\n",
    "        \n",
    "    Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "    '''\n",
    "    \n",
    "    # Evaluate the probability and select the new rows\n",
    "    rows_prob = np.array(dfCent.rdd\\\n",
    "                .map(lambda row: distance(row,bCent_ultimate.value)).collect())\n",
    "    # Forse pensare ad un modo alternativo di fare questo conto qui.... crescita lineare di bCent_ultimate\n",
    "    \n",
    "    # Sample new weighted random center\n",
    "    another_index = np.random.choice(a=range(wx.shape[0]), size=1, p = rows_prob*wx/(rows_prob@wx) ) \n",
    "    another_center = getrows(dfCent, another_index).collect()\n",
    "    \n",
    "    # Update the broadcast\n",
    "    bCent_ultimate = sc.broadcast(another_center + bCent_ultimate.value)\n",
    "\n",
    "    # For consistency reason:    \n",
    "    # Update minimum cost\n",
    "    dfCent = dfCent.select('*').\\\n",
    "    withColumn('minimum_cost', least('minimum_cost', sum((col(colname)-another_center[0][colname])**2 for colname in colnames) )).cache()\n",
    "    \n",
    "    # Evaluate new cost\n",
    "    phi0 = np.log(dfCent.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "    \n",
    "    ultimate_sample_n = len(bCent_ultimate.value)\n",
    "    print(\"i: \",ultimate_sample_n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "## k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_df = clean_X.withColumn(\"closest\", lit(None).cast(StringType()))\n",
    "\n",
    "for centro in range(k):\n",
    "    print(centro)\n",
    "    final_df = final_df.withColumn(\"closest\",\n",
    "            when((sum((col(colname)-bCent_ultimate.value[centro][colname])**2 for colname in colnames) == col(\"minimum_cost\")) & (final_df[\"closest\"].isNull()), str(centro))\n",
    "            .otherwise(final_df[\"closest\"])).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Update new centers:\n",
    "new_centers = final_df.groupBy('closest').agg(*[mean(c).alias(c) for c in colnames]).drop('closest').collect()\n",
    "bCent_ultimate = sc.broadcast(new_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Update the minimum cost function\n",
    "clean_X = clean_X.select('*').withColumn('minimum_cost', least(*[reduce(add, [(col(colname)-new_centers[center][colname])**2 for colname in colnames]) for center in range(len(new_centers))] )).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add indexes to clean_X\n",
    "clean_X = clean_X.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "# Loop\n",
    "n = 0\n",
    "phi1 = phi0\n",
    "still_different = True\n",
    "eps = 1e-4\n",
    "\n",
    "while still_different:\n",
    "\n",
    "    # # Create the weights and switch to indexed dataframe\n",
    "    # weight_X = clean_X.rdd.map(lambda row: find_closest_center(row,bCent_ultimate)).collect()\n",
    "    # dfw = spark.createDataFrame(pd.DataFrame({'closest':weight_X})) #?ocio che sarà lungo con tutto il dataset in uso\n",
    "    # dfw = dfw.withColumn(\"index\",row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    # # Join with the initial clean_X\n",
    "    # final_df = clean_X.join(dfw, on=\"index\").drop('index')\n",
    "\n",
    "    final_df = clean_X.withColumn(\"closest\", lit(None).cast(StringType()))\n",
    "\n",
    "    for centro in range(k):\n",
    "        print(centro)\n",
    "        final_df = final_df.withColumn(\"closest\",\n",
    "                when((sum((col(colname)-bCent_ultimate.value[centro][colname])**2 for colname in colnames) == col(\"minimum_cost\")) & (final_df[\"closest\"].isNull()), str(centro))\n",
    "                .otherwise(final_df[\"closest\"])).cache()\n",
    "\n",
    "    # Update new centers:\n",
    "    new_centers = final_df.groupBy('closest').agg(*[mean(c).alias(c) for c in colnames]).drop('closest').collect()\n",
    "    bCent_ultimate = sc.broadcast(new_centers)\n",
    "\n",
    "    # Update the minimum cost function\n",
    "    clean_X = clean_X.select('*').withColumn('minimum_cost', least(*[reduce(add, [(col(colname)-new_centers[center][colname])**2 for colname in colnames]) for center in range(len(new_centers))] )).cache()\n",
    "\n",
    "    # Evaluate new cost\n",
    "    phi1 = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "    print('Cost variation:', abs(phi1 - phi0)/phi0)\n",
    "    if abs(phi1 - phi0)/phi0 < eps:\n",
    "        still_different = False\n",
    "    else:\n",
    "        phi0 = phi1\n",
    "\n",
    "    n += 1\n",
    "    \n",
    "print('It took', n, 'iterations to converge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_np = clean_X.select(\"src_bytes\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_np = clean_X.select(\"dst_bytes\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(src_np[1:], dst_np[1:])\n",
    "\n",
    "for i in range(len(bCent_ultimate.value)):\n",
    "   plt.scatter(bCent_ultimate.value[i][\"src_bytes\"],bCent_ultimate.value[i][\"dst_bytes\"],color='black', alpha = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "- Prendere la lista dei centroidi e farla diventare un distributed dataset\n",
    "- Fare una NUOVA lista di centroidi \n",
    "- for i in range(k):\n",
    "    pesca un singolo dato con probabilità d(x)/phi --> come fare?\n",
    "    aggiungilo alla lista\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "    - Capire come scegliere l\n",
    "    - Fare la normalizzazione dei dati (se la facciamo)\n",
    "    - Capire se abbiamo ottimizzato nel modo giusto\n",
    "    - Implementare tutto k means (optional)\n",
    "    - Creare un file gemello con un dataframe visualizzabile e vedere se ha senso\n",
    "    - Iniziare a fare i benchmark su cloud veneto\n",
    "    - Grafici etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop worker and master\n",
    "Stop the running Spark context (sc) and Spark session (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
