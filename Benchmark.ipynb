{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b17926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/01 17:10:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"prova iniziale\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44b78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f27f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.datasets #va installato\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import least\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d54a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.86 s, sys: 1.59 s, total: 7.45 s\n",
      "Wall time: 7.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prova =  sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True)\n",
    "X = prova.data\n",
    "\n",
    "n_cols = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc0f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve a single row.... copied from the internet\n",
    "def getrows(df, rownums=None): \n",
    "    return df.rdd.zipWithIndex().filter(lambda x: x[1] in rownums).map(lambda x: x[0])\n",
    "\n",
    "# Define the function for the distance between the cluster centers and the pandas dataframe\n",
    "def distance(xrow, centers, num_cols=n_cols):\n",
    "    '''Distance between a dataframe.row\n",
    "    and the broadcasted value list of centers\n",
    "    in the form of dataframe.row.value:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows).value\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    x = np.array(xrow)[:num_cols]\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])[:num_cols]\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.min(the_ds)\n",
    "\n",
    "def find_closest_center(xrow,broadC):\n",
    "    '''Distance between a dataframe.row\n",
    "    and the broadcasted list of centers\n",
    "    in the form of dataframe.row:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows)\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    x = np.array(xrow)\n",
    "    centers = broadC.value\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.argmin(the_ds)\n",
    "\n",
    "def evaluate_l(log_phi, k, G):\n",
    "    return G * k/log_phi # G = over-oversampling factor\n",
    "\n",
    "# Function to select a row based on its probability\n",
    "def select_row(x):\n",
    "    if x > np.random.uniform(low = 0, high = 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def if_greater(xrow, centers):\n",
    "    return min(distance(xrow, centers, 38), xrow.__getitem__('minimum_cost'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparameters\n",
    "# ks =      [4,   20,    60,    80,     200,    300]\n",
    "# len_dfs = [100, 1_000, 5_000, 10_000, 50_000, 100_000]\n",
    "\n",
    "ks =      [4]\n",
    "len_dfs = [2_023]\n",
    "\n",
    "# ks =      [4,   4,     4,     4,      4,      4,       4]\n",
    "# len_dfs = [100, 1_000, 5_000, 10_000, 50_000, 100_000, 494_021]\n",
    "Gs = list(np.full(len(ks), 3)) # Giosu factor\n",
    "\n",
    "cycle = list(zip(ks, Gs, len_dfs))\n",
    "times = []\n",
    "\n",
    "for step in cycle:\n",
    "    time_zero = time.time()\n",
    "    k = step[0]\n",
    "    G = step[1]\n",
    "    len_df = step[2]\n",
    "\n",
    "    X_smaller = X.iloc[random.sample(range(0, len(X.index)), len_df)]\n",
    "    spark_X = spark.createDataFrame(X_smaller)\n",
    "    spark_X.persist() # persist in the memory\n",
    "\n",
    "    n_rows = spark_X.count()\n",
    "    n_cols = len(getrows(spark_X, rownums=[0]).collect()[0])\n",
    "\n",
    "    col_type = np.array(spark_X.dtypes)\n",
    "    types = col_type[:,1] \n",
    "    colnames = col_type[:,0]\n",
    "    clean_X = spark_X.select([col(colnames[i]) for i in range(len(colnames)) if not types[i] == 'binary'])\n",
    "\n",
    "    colnames = list(clean_X.dtypes[i][0] for i in range(len(clean_X.dtypes)))\n",
    "\n",
    "    random_n = [np.random.randint(0, n_rows)]\n",
    "    random_sample = getrows(clean_X, random_n).collect()\n",
    "\n",
    "    clean_X = (clean_X.select('*')\n",
    "               .withColumn('minimum_cost', sum((col(colname)-random_sample[0][colname])**2 for colname in colnames)))\n",
    "\n",
    "    initial_cost = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "    n_iter = int(initial_cost)\n",
    "\n",
    "    temp = random_sample[0].asDict()\n",
    "    temp[\"minimum_cost\"] = 0\n",
    "    random_sample[0] = Row(**temp)\n",
    "\n",
    "    bCent = sc.broadcast(random_sample)\n",
    "\n",
    "    phi_iter = initial_cost\n",
    "    l = evaluate_l(phi_iter, k, G)\n",
    "\n",
    "    i = 0\n",
    "    last_centers = 1\n",
    "    start_time = time.time()\n",
    "\n",
    "    # To be removed\n",
    "    new_centers = []\n",
    "    time_new_rows = []\n",
    "    time_broadc = []\n",
    "    time_update = []\n",
    "\n",
    "    while i < n_iter:# or len(bCent.value) < k:\n",
    "\n",
    "        '''\n",
    "        Nel ciclo for:\n",
    "            - Evaluate for each row l * d()^2 / phi\n",
    "            - Sample with that probability\n",
    "            - Broadcast centers to nodes\n",
    "            - Evaluate new cost\n",
    "\n",
    "        Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "        '''\n",
    "\n",
    "        # Evaluate the probability and select the new rows\n",
    "        new_rows = clean_X.rdd\\\n",
    "                   .filter(lambda row: select_row(np.exp(np.log(row.__getitem__('minimum_cost'))-phi_iter)*l)).collect()\n",
    "        # To be removed\n",
    "        time_new_rows.append((time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "#         # Evaluate the probability and select the new rows\n",
    "#         new_rows = clean_X.select('*').withColumn('random_number', rand(seed=4245))\\\n",
    "#                           .filter(col('random_number') < col('minimum_cost')).drop('random_number').collect()\n",
    "#         # To be removed\n",
    "#         time_new_rows.append((time.time() - start_time))\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        # Update the broadcast\n",
    "        bCent = sc.broadcast(bCent.value + new_rows)\n",
    "        # To be removed\n",
    "        time_broadc.append((time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Update the minimum distance\n",
    "        if len(new_rows) == 1:\n",
    "            clean_X = clean_X.select('*').withColumn('minimum_cost', least('minimum_cost', sum((col(colname)-bCent.value[0][colname])**2 for colname in colnames) ))\n",
    "\n",
    "        elif len(new_rows) > 1:\n",
    "            clean_X = clean_X.select('*').withColumn('dummy', least(*[sum((col(colname)-bCent.value[center][colname])**2 for colname in colnames) for center in range(len(new_rows))] )).\\\n",
    "                      withColumn('minimum_cost', least('minimum_cost', 'dummy')).\\\n",
    "                      drop('dummy')\n",
    "\n",
    "        last_centers = len(bCent.value)\n",
    "\n",
    "        # Evaluate new cost\n",
    "        phi_iter = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "\n",
    "        # To be removed\n",
    "        time_update.append((time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "        i += 1\n",
    "        # To be removed\n",
    "        new_centers.append(len(new_rows))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    ax1.plot(new_centers, 'o--', label='# new centers')\n",
    "    ax1.set_xlabel('# iteration')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(time_new_rows, 'o--', label='new rows')\n",
    "    ax2.plot(time_broadc, 'o--', label='broadcast')\n",
    "    ax2.plot(time_update, 'o--', label='update')\n",
    "    ax2.set_xlabel('# iteration')\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "\n",
    "    figname = 'figures/fig_k' + str(step[0]) + '_G' + str(step[1]) + '_len' + str(step[2])\n",
    "    plt.savefig(figname)\n",
    "    \n",
    "    times.append(time.time() - time_zero)\n",
    "    print('\\nIn configuration (k, G, len_df) =', step, ' time spent is', times[-1], 'seconds')\n",
    "    print('Found', len(bCent.value), 'centers in', n_iter, 'iterations')\n",
    "    print('Dataframe had', n_rows, 'rows and', n_cols, 'columns')\n",
    "    \n",
    "    bCent.destroy()\n",
    "    spark_X.unpersist() # to unpersist the memory in the workers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21237eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 419 ms, total: 2min 47s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_smaller = X.iloc[random.sample(range(0, len(X.index)), 494_021)]\n",
    "spark_X = spark.createDataFrame(X_smaller)\n",
    "\n",
    "col_type = np.array(spark_X.dtypes)\n",
    "types = col_type[:,1] \n",
    "colnames = col_type[:,0]\n",
    "clean_X = spark_X.select([col(colnames[i]) for i in range(len(colnames)) if not types[i] == 'binary'])\n",
    "colnames = list(clean_X.dtypes[i][0] for i in range(len(clean_X.dtypes)))\n",
    "\n",
    "clean_X = (clean_X.select('*')\n",
    "           .withColumn('minimum_cost', sum((col(colname))**2 for colname in colnames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64b5c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/01 17:26:20 WARN TaskSetManager: Stage 19 contains a task of very large size (33178 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "initial_cost = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dea0d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/01 17:26:45 WARN TaskSetManager: Stage 22 contains a task of very large size (33178 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 ms, sys: 9.91 ms, total: 24.2 ms\n",
      "Wall time: 6.46 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spark.driver.maxResultSize\n",
    "# spark.executor.memory\n",
    "new_rows = clean_X.rdd.filter(lambda row: select_row(0.5*row['minimum_cost']/np.exp(initial_cost))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be80366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/01 17:26:52 WARN TaskSetManager: Stage 23 contains a task of very large size (33178 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 192 µs, total: 31.3 ms\n",
      "Wall time: 7.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_rows = clean_X.rdd.filter(lambda row: select_row(0.5*row.__getitem__('minimum_cost')/np.exp(initial_cost))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6febd04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/01 17:26:59 WARN TaskSetManager: Stage 24 contains a task of very large size (33178 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 ms, sys: 224 µs, total: 33.2 ms\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_rows = clean_X.select('*').withColumn('random_number', rand(seed=4245))\\\n",
    "                  .filter(col('random_number') < 0.5*col('minimum_cost')/np.exp(initial_cost)).drop('random_number').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e011f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(duration=2, src_bytes=693375640, dst_bytes=0, land=0, wrong_fragment=0, urgent=0, hot=1, num_failed_logins=0, logged_in=0, num_compromised=0, root_shell=0, su_attempted=0, num_root=0, num_file_creations=0, num_shells=0, num_access_files=0, num_outbound_cmds=0, is_host_login=0, is_guest_login=0, count=57, srv_count=3, serror_rate=0.79, srv_serror_rate=0.67, rerror_rate=0.21, srv_rerror_rate=0.33, same_srv_rate=0.05, diff_srv_rate=0.39, srv_diff_host_rate=0.0, dst_host_count=255, dst_host_srv_count=3, dst_host_same_srv_rate=0.01, dst_host_diff_srv_rate=0.09, dst_host_same_src_port_rate=0.22, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.18, dst_host_srv_serror_rate=0.67, dst_host_rerror_rate=0.05, dst_host_srv_rerror_rate=0.33, minimum_cost=4.807697781454779e+17)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
