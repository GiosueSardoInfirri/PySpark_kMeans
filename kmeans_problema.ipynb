{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k means ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/30 10:50:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"prova iniziale\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0df826e5e174:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>prova iniziale</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7facbc59f460>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0df826e5e174:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>prova iniziale</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=prova iniziale>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# print its status\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn # to be run at the launch of \"docker compose up\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn.datasets #va installato\n",
    "\n",
    "# from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "k = 20\n",
    "G = 3 # Giosu factor\n",
    "len_df = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe directly from sklearn\n",
    "prova =  sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True)\n",
    "X = prova.data\n",
    "\n",
    "# prova = sc.parallelize(sklearn.datasets.fetch_kddcup99(percent10 = True, as_frame = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494016</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>310</td>\n",
       "      <td>1881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494017</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>282</td>\n",
       "      <td>2286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494018</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>203</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494019</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>291</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494020</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>219</td>\n",
       "      <td>1234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494021 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration protocol_type  service   flag src_bytes dst_bytes land  \\\n",
       "0             0        b'tcp'  b'http'  b'SF'       181      5450    0   \n",
       "1             0        b'tcp'  b'http'  b'SF'       239       486    0   \n",
       "2             0        b'tcp'  b'http'  b'SF'       235      1337    0   \n",
       "3             0        b'tcp'  b'http'  b'SF'       219      1337    0   \n",
       "4             0        b'tcp'  b'http'  b'SF'       217      2032    0   \n",
       "...         ...           ...      ...    ...       ...       ...  ...   \n",
       "494016        0        b'tcp'  b'http'  b'SF'       310      1881    0   \n",
       "494017        0        b'tcp'  b'http'  b'SF'       282      2286    0   \n",
       "494018        0        b'tcp'  b'http'  b'SF'       203      1200    0   \n",
       "494019        0        b'tcp'  b'http'  b'SF'       291      1200    0   \n",
       "494020        0        b'tcp'  b'http'  b'SF'       219      1234    0   \n",
       "\n",
       "       wrong_fragment urgent hot  ... dst_host_count dst_host_srv_count  \\\n",
       "0                   0      0   0  ...              9                  9   \n",
       "1                   0      0   0  ...             19                 19   \n",
       "2                   0      0   0  ...             29                 29   \n",
       "3                   0      0   0  ...             39                 39   \n",
       "4                   0      0   0  ...             49                 49   \n",
       "...               ...    ...  ..  ...            ...                ...   \n",
       "494016              0      0   0  ...             86                255   \n",
       "494017              0      0   0  ...              6                255   \n",
       "494018              0      0   0  ...             16                255   \n",
       "494019              0      0   0  ...             26                255   \n",
       "494020              0      0   0  ...              6                255   \n",
       "\n",
       "       dst_host_same_srv_rate dst_host_diff_srv_rate  \\\n",
       "0                         1.0                    0.0   \n",
       "1                         1.0                    0.0   \n",
       "2                         1.0                    0.0   \n",
       "3                         1.0                    0.0   \n",
       "4                         1.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "494016                    1.0                    0.0   \n",
       "494017                    1.0                    0.0   \n",
       "494018                    1.0                    0.0   \n",
       "494019                    1.0                    0.0   \n",
       "494020                    1.0                    0.0   \n",
       "\n",
       "       dst_host_same_src_port_rate dst_host_srv_diff_host_rate  \\\n",
       "0                             0.11                         0.0   \n",
       "1                             0.05                         0.0   \n",
       "2                             0.03                         0.0   \n",
       "3                             0.03                         0.0   \n",
       "4                             0.02                         0.0   \n",
       "...                            ...                         ...   \n",
       "494016                        0.01                        0.05   \n",
       "494017                        0.17                        0.05   \n",
       "494018                        0.06                        0.05   \n",
       "494019                        0.04                        0.05   \n",
       "494020                        0.17                        0.05   \n",
       "\n",
       "       dst_host_serror_rate dst_host_srv_serror_rate dst_host_rerror_rate  \\\n",
       "0                       0.0                      0.0                  0.0   \n",
       "1                       0.0                      0.0                  0.0   \n",
       "2                       0.0                      0.0                  0.0   \n",
       "3                       0.0                      0.0                  0.0   \n",
       "4                       0.0                      0.0                  0.0   \n",
       "...                     ...                      ...                  ...   \n",
       "494016                  0.0                     0.01                  0.0   \n",
       "494017                  0.0                     0.01                  0.0   \n",
       "494018                 0.06                     0.01                  0.0   \n",
       "494019                 0.04                     0.01                  0.0   \n",
       "494020                  0.0                     0.01                  0.0   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "0                           0.0  \n",
       "1                           0.0  \n",
       "2                           0.0  \n",
       "3                           0.0  \n",
       "4                           0.0  \n",
       "...                         ...  \n",
       "494016                      0.0  \n",
       "494017                      0.0  \n",
       "494018                      0.0  \n",
       "494019                      0.0  \n",
       "494020                      0.0  \n",
       "\n",
       "[494021 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the dataframe (to be deleted for a bigger dataframe) (TO BE DELETED)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spark dataframe\n",
    "And then acquire important informations about the dataframe: # columns, # rows, # partitions, the schema, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/bin/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Create the spark dataframe with a smaller chunk of data (just to work easily)\n",
    "X_smaller = X.iloc[random.sample(range(1, len(X.index)), len_df)]\n",
    "spark_X = spark.createDataFrame(X_smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of partitions the DataFrame is divided into the three workers yet\n",
    "spark_X.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that is able to select the single row in the dataframe\n",
    "def getrows(df, rownums=None): # copied from the internet\n",
    "    return df.rdd.zipWithIndex().filter(lambda x: x[1] in rownums).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns in the dataframe\n",
    "n_rows = spark_X.count()\n",
    "n_cols = len(getrows(spark_X, rownums=[0]).collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the non-numerical values in an easy way\n",
    "# clean_X = spark_X.select('*').drop('protocol_type', 'service', 'flag')\n",
    "# clean_X.printSchema()\n",
    "\n",
    "# Delete the non-numerical values in a reusable way\n",
    "col_type = np.array(spark_X.dtypes)\n",
    "types = col_type[:,1] \n",
    "colnames = col_type[:,0]\n",
    "clean_X = spark_X.select([col(colnames[i]) for i in range(len(colnames)) if not types[i] == 'binary'])\n",
    "# clean_X.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical columns\n",
    "\n",
    "i = 0\n",
    "means = np.zeros(len(clean_X.columns))\n",
    "devstd = np.zeros(len(clean_X.columns))\n",
    "# Calculate mean and variance for each column\n",
    "for column in clean_X.columns:\n",
    "    means[i] = float(clean_X.select(mean(column)).head()[0])\n",
    "    devstd[i] = float(clean_X.select(stddev(column)).head()[0])\n",
    "    i = i+1\n",
    "\n",
    "# norm_X = clean_X\n",
    "means, devstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE NORMALIZED!!\n",
    "\n",
    "# Step 0: Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function for the distance between the cluster centers and the pandas dataframe\n",
    "# def distance(xrow,broadC):\n",
    "#     '''Distance between a dataframe.row\n",
    "#     and the broadcasted list of centers\n",
    "#     in the form of dataframe.row:\n",
    "    \n",
    "#     broadC = sc.broadcast(center_rows)\n",
    "#     xrow = clean_X.collect()[1]\n",
    "#     '''\n",
    "#     x = np.array(xrow)\n",
    "#     centers = broadC.value\n",
    "#     the_ds = np.zeros(len(centers))\n",
    "    \n",
    "#     for c in range(len(centers)):\n",
    "#         c_array = np.array(centers[c])\n",
    "#         dist2 = np.linalg.norm(x - c_array)**2\n",
    "#         the_ds[c] = dist2\n",
    "        \n",
    "#     return np.min(the_ds)\n",
    "\n",
    "# def find_closest_center(xrow,broadC):\n",
    "#     '''Distance between a dataframe.row\n",
    "#     and the broadcasted list of centers\n",
    "#     in the form of dataframe.row:\n",
    "    \n",
    "#     broadC = sc.broadcast(center_rows)\n",
    "#     xrow = clean_X.collect()[1]\n",
    "#     '''\n",
    "#     x = np.array(xrow)\n",
    "#     centers = broadC.value\n",
    "#     the_ds = np.zeros(len(centers))\n",
    "    \n",
    "#     for c in range(len(centers)):\n",
    "#         c_array = np.array(centers[c])\n",
    "#         dist2 = np.linalg.norm(x - c_array)**2\n",
    "#         the_ds[c] = dist2\n",
    "        \n",
    "#     return np.argmin(the_ds)\n",
    "\n",
    "# # Define the function for the logarithm of the cost phi\n",
    "# def phi(df,centers ):\n",
    "#     '''logarithm of the cost phi'''\n",
    "#     return np.log( df.rdd.map(lambda row: distance(row,centers)).reduce(lambda x,y: x + y) )\n",
    "\n",
    "# # Is this reasonable?? --> maybe we should cast it to int ?\n",
    "# def evaluate_l(log_phi, k, G):\n",
    "#     return G * k/log_phi # G = over-oversampling factor\n",
    "\n",
    "# # Function to select a row based on its probability\n",
    "# def select_row(x):\n",
    "#     if x > np.random.uniform(low = 0, high = 1):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED FUNCTIONS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define the function for the distance between the cluster centers and the pandas dataframe\n",
    "# @udf('float')\n",
    "def distance(xrow, centers, delete_last=False):\n",
    "    '''Distance between a dataframe.row\n",
    "    and the broadcasted value list of centers\n",
    "    in the form of dataframe.row.value:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows).value\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    if delete_last:\n",
    "        x = np.array(xrow)[:n_cols]\n",
    "    else:\n",
    "        x = np.array(xrow)\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])[:n_cols]\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.min(the_ds)\n",
    "\n",
    "def find_closest_center(xrow,broadC):\n",
    "    '''Distance between a dataframe.row\n",
    "    and the broadcasted list of centers\n",
    "    in the form of dataframe.row:\n",
    "    \n",
    "    broadC = sc.broadcast(center_rows)\n",
    "    xrow = clean_X.collect()[1]\n",
    "    '''\n",
    "    x = np.array(xrow)\n",
    "    centers = broadC.value\n",
    "    the_ds = np.zeros(len(centers))\n",
    "    \n",
    "    for c in range(len(centers)):\n",
    "        c_array = np.array(centers[c])\n",
    "        dist2 = np.linalg.norm(x - c_array)**2\n",
    "        the_ds[c] = dist2\n",
    "        \n",
    "    return np.argmin(the_ds)\n",
    "\n",
    "# # Define the function for the logarithm of the cost phi\n",
    "# def phi(df,centers ):\n",
    "#     '''logarithm of the cost phi\n",
    "#     Remember:\n",
    "#     centers = broadc.value'''\n",
    "#     return np.log( df.rdd.map(lambda row: distance(row,centers)).reduce(lambda x,y: x + y) )\n",
    "\n",
    "# Is this reasonable?? --> maybe we should cast it to int ?\n",
    "def evaluate_l(log_phi, k, G):\n",
    "    return G * k/log_phi # G = over-oversampling factor\n",
    "\n",
    "# Function to select a row based on its probability\n",
    "def select_row(x):\n",
    "    if x > np.random.uniform(low = 0, high = 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def if_greater(xrow, centers):\n",
    "    return min(distance(xrow, centers, delete_last=True), xrow.__getitem__('minimum_cost'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is NOT $\\Phi$. This is $log(\\Phi)$. Pay attention budeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose a random sample from the dataset\n",
    "This is the required step to begin the algorithm (doesn't need to be parallelized, since it is a select task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first sample randomly: select the random row\n",
    "random_n = [np.random.randint(0, n_rows)]\n",
    "random_sample = getrows(clean_X, random_n).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the row over the workers\n",
    "bCent = sc.broadcast(random_sample)\n",
    "\n",
    "# To show the content of the broadcast:\n",
    "# b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initial cost\n",
    "Then we evaluate the cost function (sum of the squares after the first selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(clean_X.dtypes[i][0] for i in range(len(clean_X.dtypes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X = (clean_X.select('*')\n",
    "           .withColumn('minimum_cost', sum((col(colname)-random_sample[0][colname])**2 for colname in colnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cost\n",
    "# initial_cost = phi(clean_X, bCent)\n",
    "initial_cost = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "# initial_cost = clean_X.select(['minimum_cost']).rdd.map(lambda x : x).reduce(sum)\n",
    "initial_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: evaluate number of iterations\n",
    "\n",
    "After having evaluated $\\log \\phi$, we find a number of iterations which is of the same order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(initial_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = int(initial_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: For loop\n",
    "Implement the for loop in order to evaluate probabilities and choose new centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_iter = initial_cost\n",
    "l = evaluate_l(phi_iter, k, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# i = 0\n",
    "# start_time = time.time()\n",
    "\n",
    "# while i < n_iter or len(bCent.value) < k:\n",
    "# # while i < 1:\n",
    "#     '''\n",
    "#     Nel ciclo for:\n",
    "#         - Evaluate for each row l * d()^2 / phi\n",
    "#         - Sample with that probability\n",
    "#         - Broadcast centers to nodes\n",
    "#         - Evaluate new cost\n",
    "        \n",
    "#     Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "#     '''\n",
    "\n",
    "#     # Evaluate the probability and select the new rows\n",
    "#     #probabilities = clean_X.rdd.map(lambda row: np.exp(np.log(distance(row,bCent))-phi_iter)*l)\n",
    "#     new_rows = clean_X.rdd\\\n",
    "#     .filter(lambda row: select_row(np.exp(np.log(distance(row,bCent))-phi_iter)*l)).collect()\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Update the broadcast\n",
    "#     bCent = sc.broadcast(bCent.value + new_rows)\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Evaluate new cost\n",
    "#     phi_iter =  phi(clean_X, bCent)\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     i += 1\n",
    "#     print(\"\\n at iteration\", i ,\", #values: \",len(bCent.value), \" \\n\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each row evaluate distance from new points --> if it's less than mindistance substitute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 nuovi centri --> fai una colonna per ogni centro --> modifica --> droppa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best to ship this function to all executors in case of huge datasets\n",
    "def chooseNewDistance(groupedRows, index):\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for row in groupedRows:\n",
    "    #calcola nuova distanza da centri --> vedi se è minore dell'ultima entrata --> sostituisci se serve\n",
    "        new_distance = distance(row, bCent.value[index: ], True)\n",
    "        best_distance = new_distance\n",
    "        if row.minimum_cost < best_distance:\n",
    "            best_distance = row.minimum_cost\n",
    "\n",
    "\n",
    "    res.append([item for item in row][0:n_cols] + [best_distance])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# OPTIMIZED LOOP\n",
    "\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "i = 0\n",
    "last_centers = 1\n",
    "start_time = time.time()\n",
    "\n",
    "index = 1\n",
    "\n",
    "while i < n_iter:# or len(bCent.value) < k:\n",
    "\n",
    "    '''\n",
    "    Nel ciclo for:\n",
    "        - Evaluate for each row l * d()^2 / phi\n",
    "        - Sample with that probability\n",
    "        - Broadcast centers to nodes\n",
    "        - Evaluate new cost\n",
    "        \n",
    "    Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "    '''\n",
    "    \n",
    "    # Evaluate the probability and select the new rows\n",
    "    new_rows = clean_X.rdd\\\n",
    "               .filter(lambda row: select_row(np.exp(np.log(row.__getitem__('minimum_cost'))-phi_iter)*l)).collect()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Update the broadcast\n",
    "    bCent = sc.broadcast(bCent.value + new_rows)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Update the minimum distance\n",
    "    if len(new_rows) > 0:\n",
    "        print(\"claisjnflkjasdnf\")\n",
    "        data_vals = clean_X.rdd. \\\n",
    "    groupBy(lambda gk: 1). \\\n",
    "    flatMapValues(lambda r: chooseNewDistance(r, index)). \\\n",
    "    values()\n",
    "        \n",
    "        data_schema = clean_X.schema\n",
    "        \n",
    "        clean_X = spark.createDataFrame(data_vals, data_schema)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print('OOOOO', len(new_rows))\n",
    "        #clean_X = clean_X.withColumn('dummy', \n",
    "         #         array(sum((col(colname)-bCent.value[center][colname])**2 for colname in colnames) for center in range(len(new_rows))))\n",
    "\n",
    "        #print('OOOOO', len(new_rows))\n",
    "        #clean_X = clean_X.withColumn('dummy', least(*[col(\"dummy\")[i] for i in range(len(new_rows))]))\n",
    "        \n",
    "#       # clean_X = clean_X.withColumn('dummy',\n",
    "#        #           least(*[(sum((col(colname)-bCent.value[center][colname])**2 for colname in colnames) for center in range(len(new_rows)))[i]\n",
    "#         #          for i in range(len(new_rows))]))\n",
    "\n",
    "#          #         least(*(sum((col(colname)-bCent.value[center][colname])**2 for colname in colnames) for center in range(len(new_rows)))))\n",
    "\n",
    "#         #df_with_min = df.withColumn(\"min_value\", least(*[col(\"values\")[i] for i in range(len(data[0][1]))]))\n",
    "\n",
    "        #print('OOOOO', len(new_rows))\n",
    "        #clean_X = clean_X.withColumn('minimum_cost', least(col('minimum_cost'), col('dummy')))\n",
    "\n",
    "        #print('OOOOO', len(new_rows))\n",
    "        #clean_X = clean_X.drop('dummy')\n",
    "\n",
    "        #last_centers = len(bCent.value)\n",
    "\n",
    "#         names = ['minimum_cost']\n",
    "        \n",
    "#         for center in range(len(new_rows)):\n",
    "#             name = 'dummy_'+str(center)\n",
    "#             clean_X = clean_X.withColumn( name, sum((col(colname)-bCent.value[center][colname])**2 for colname in colnames) )\n",
    "#             names.append(name)\n",
    "            \n",
    "#         column_list = (col(n) for n in names)\n",
    "#         clean_X = clean_X.withColumn('minimum_cost', least(*column_list))\n",
    "\n",
    "#         for center in range(len(new_rows)):\n",
    "#             name = 'dummy_'+str(center)\n",
    "#             clean_X = clean_X.drop( name )\n",
    "\n",
    "#         last_centers = len(bCent.value)\n",
    "        \n",
    "        # Evaluate new cost\n",
    "        phi_iter = np.log(clean_X.agg({\"minimum_cost\": \"sum\"}).collect()[0][0])\n",
    "\n",
    "        \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    index = len(bCent.values)\n",
    "\n",
    "    i += 1\n",
    "    print(\"\\n at iteration\", i ,\", #values: \",len(bCent.value), \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent.value[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(bCent.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Select a subset of the possible centroids using k-means ++\n",
    "\n",
    "Using kmeans ++:\n",
    "\n",
    "Algorithm 1 k-means++(k) initialization.\n",
    "1: C ← sample a point uniformly at random from X\n",
    "2: while |C| < k do\n",
    "2\n",
    "3:\n",
    "Sample x ∈ X with probability dφX(x,C)\n",
    "(C)\n",
    "4:\n",
    "C ← C ∪ {"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent_list = bCent.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCent = spark.createDataFrame(bCent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampled_centers = dfCent.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCent.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent = sc.broadcast(bCent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bCent.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce size of dfCent to have only `k` centroids \n",
    "\n",
    "#### Step 1: calc weights $w_x$\n",
    "set wx to be the number of points in X closer to x than any other point in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = clean_X.rdd.map(lambda row: (find_closest_center(row,bCent),1)).reduceByKey(lambda x,y: x+y).takeOrdered(over_sampled_centers) #o lambda x,y: x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = (np.array(wx)[:,1]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx /= np.sum(wx)\n",
    "wx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use weighted `k-means++`\n",
    "    2.1 Draw 1 random center\n",
    "    2.2 update cost function\n",
    "    2.3 repeat 1-2 until happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_center = np.random.choice(a=range(wx.shape[0]), size=1, p=wx) \n",
    "#first point sampled uniformly wrt distance\n",
    "first_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCent_ultimate = sc.broadcast(getrows(dfCent,first_center).collect())\n",
    "bCent_ultimate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi0 = phi(dfCent, bCent_ultimate)\n",
    "phi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bCent_ultimate.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "while i < k: #n_iter or len(bCent.value) < k:\n",
    "    '''\n",
    "    Nel ciclo for:\n",
    "        - Evaluate for each row l * d()^2 / phi\n",
    "        - Sample with that probability\n",
    "        - Broadcast centers to nodes\n",
    "        - Evaluate new cost\n",
    "        \n",
    "    Ricordiamoci che distance è già al quadrato e che phi è il logaritmo\n",
    "    '''\n",
    "    \n",
    "    # Evaluate the probability and select the new rows\n",
    "    #probabilities = clean_X.rdd.map(lambda row: np.exp(np.log(distance(row,bCent))-phi_iter)*l)\n",
    "    rows_prob = np.array(dfCent.rdd\\\n",
    "    .map(lambda row: (np.exp(np.log(distance(row,bCent_ultimate))-phi0))).collect())\n",
    "    \n",
    "    #sample new random center\n",
    "    if i == 0:\n",
    "        print('rows_prob@wx \\n',rows_prob@wx, '\\n')\n",
    "    \n",
    "    another_index = np.random.choice(a=range(wx.shape[0]), size=1, p = rows_prob*wx/(rows_prob@wx) ) \n",
    "    \n",
    "    print(i ,'step: another_index = ', another_index,'\\n')\n",
    "    \n",
    "    another_center = getrows(dfCent, another_index).collect()\n",
    "    \n",
    "    # Update the broadcast\n",
    "    bCent_ultimate = sc.broadcast(bCent_ultimate.value + another_center)\n",
    "    \n",
    "    # Evaluate new cost\n",
    "    phi0 = phi(dfCent, bCent_ultimate)\n",
    "    \n",
    "    i += 1\n",
    "    print(\"\\n i: \",i,\"; bC.val =\",bCent_ultimate.value, \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rows_prob*wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Prendere la lista dei centroidi e farla diventare un distributed dataset\n",
    "- Fare una NUOVA lista di centroidi \n",
    "- for i in range(k):\n",
    "    pesca un singolo dato con probabilità d(x)/phi --> come fare?\n",
    "    aggiungilo alla lista\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "    - Capire come scegliere l\n",
    "    - Fare la normalizzazione dei dati (se la facciamo)\n",
    "    - Capire se abbiamo ottimizzato nel modo giusto\n",
    "    - Implementare tutto k means (optional)\n",
    "    - Creare un file gemello con un dataframe visualizzabile e vedere se ha senso\n",
    "    - Iniziare a fare i benchmark su cloud veneto\n",
    "    - Grafici etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop worker and master\n",
    "Stop the running Spark context (sc) and Spark session (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
